# Assignment Feedback: Week 04 Dimensionality Reduction

**Student:** zseidel18
**Raw Score:** 39/40 (97.5%)
**Course Points Earned:** 72.0

---

## Problem Breakdown

### Exercise 2 (10/10 = 100.0%)

**Part ex1-part1** (ex1-part1.code): 4/4 points

_Feedback:_ Good job: sensible subsampling, PCA pre-reduction, t-SNE to 2D, and clear scatter with labels/colorbar. This meets the goal. Just ensure X/y refer to the MNIST data and remember plt.show() if needed in your environment.

**Part ex1-part2** (ex1-part2.code): 3/3 points

_Feedback:_ Good job: you reduced with PCA, applied t-SNE, trained KNN, and reported accuracy. Note the acknowledged data leakage from fitting t-SNE on combined train+test; prefer fitting on train only (though t-SNE lacks transform). Otherwise solid implementation.

**Part ex1-part3** (ex1-part3.code): 3/3 points

_Feedback:_ Good job: you used UMAP, trained KNN on the UMAP-transformed train set, and evaluated accuracy on the transformed test set. No data leakage in the final pipeline. Minor: the initial fit_transform on X is redundant; you can remove it for efficiency.

---

### Exercise 4 (14/14 = 100.0%)

**Part ex2-part1** (ex2-part1.code): 7/7 points

_Feedback:_ Well done. You applied PCA with multiple component counts, trained KNN, and reported accuracies. Your visualizations for 1–3 PCs are appropriate and informative. For refinement, consider inspecting explained_variance_ratio_ and plotting on train-only data, but no points off.

**Part ex2-part2** (ex2-part2.code): 7/7 points

_Feedback:_ Strong work: you implemented UMAP with varied n_components/n_neighbors/min_dist, trained KNN on embeddings, evaluated accuracy, and visualized 1D/2D/3D cases. Explanation addresses PCA vs UMAP reasonably. No issues noted. Full credit.

---

### Exercise 1 (15/16 = 93.8%)

**Part pipeline-part2** (pipeline-part2.code): 4/4 points

_Feedback:_ Good job. You fit PCA with 40 components and plotted the explained_variance_ratio_ over components 1–40. This correctly produces a scree plot of proportion of variance explained. Labels and formatting are acceptable for this task.

**Part pipeline-part3** (pipeline-part3.code): 4/4 points

_Feedback:_ Correct approach. Using PCA(n_components=0.95) and reporting n_components_ correctly computes the number of components to explain 95% variance. Full credit.

**Part pipeline-part4** (pipeline-part4.code): 3/4 points

_Feedback:_ Good reconstruction and visualization pipeline. However, you hard-coded 149 components instead of using the number identified in Step 4 (e.g., pca_95.n_components_). Minor redundancy using fit_transform then transform. Otherwise correct approach.

**Part pipeline-part5** (pipeline-part5.code): 4/4 points

_Feedback:_ Good job. You ran KNN on original data and with PCA preserving ~80% variance, fitting PCA on train and transforming test, and reported both accuracies. Choice of k=10 is fine. Meets the task’s objectives.

---

## Additional Information

This feedback was automatically generated by the autograder.

**Generated:** 2025-10-28 19:51:53 UTC

If you have questions about your grade, please reach out to the instructor.