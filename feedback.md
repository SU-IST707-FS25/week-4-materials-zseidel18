# Assignment Feedback: Week 4: Dimensionality Reduction

**Student:** zseidel18
**Total Score:** 18/33 (54.5%)

**Grade Category:** F (Failing)

---

## Problem Breakdown

### Exercise 1 (5/16 = 31.2%)

**Part pipeline-part2** (pipeline-part2.code): 2/4 points

_Feedback:_ You correctly applied PCA and produced a scree plot. However, the task required a 2D scatter of the first two principal components colored by class labels. No 2D scatter or coloring by y was provided. Plot X_pca[:,0] vs X_pca[:,1] with c=y to meet the spec.

**Part pipeline-part3** (pipeline-part3.code): 1/4 points

_Feedback:_ You computed components for 95% variance, but the task was to calculate and visualize a scree plot for the first 40 components with percent variance explained on the y-axis. No plot was produced and you didn’t use the prior pca (40 comps). Plot pca.explained_variance_ratio_[:40].

**Part pipeline-part4** (pipeline-part4.code): 1/4 points

_Feedback:_ You used PCA and reconstruction, but you didn’t calculate or report the number of components needed to explain 95% variance as asked. Use explained_variance_ratio_ or PCA(n_components=0.95) and print pca_95.n_components_. Also, inverse_transform expects 2D; pass X_train[0:1], not

**Part pipeline-part5** (pipeline-part5.code): 1/4 points

_Feedback:_ This does not address Step 5. You should reduce a digit using the dimensions from Step 4 (e.g., your 149 comps), inverse_transform it, and plot with plot_mnist_digit. Your code runs KNN and PCA for classification instead, with no visualization or reconstruction.

---

### Exercise 2 (10/10 = 100.0%)

**Part ex1-part1** (ex1-part1.code): 4/4 points

_Feedback:_ Good solution: subsamples data, reduces to 50D with PCA, then applies 2D t-SNE and visualizes with labels and colorbar. Sensible params (init='pca', random_state) and clear plot. Ensure X/y refer to MNIST data defined earlier. Full credit.

**Part ex1-part2** (ex1-part2.code): 3/3 points

_Feedback:_ Good job: you applied t-SNE (after PCA) and evaluated KNN accuracy, and you correctly noted the data leakage from fitting t-SNE on combined train+test. For a fair eval, avoid leakage (e.g., use a validation split or acknowledge it as exploratory).

**Part ex1-part3** (ex1-part3.code): 3/3 points

_Feedback:_ Good job computing KNN accuracy. You properly fit UMAP on the training set and transformed the test set before KNN, avoiding leakage. Minor: the initial fit_transform on X is redundant and can be removed to save time.

---

### Exercise 4 (3/7 = 42.9%)

**Part ex2-part1** (ex2-part1.code): 0/0 points

_Feedback:_ Good PCA exploration: varied components (1–3), evaluated KNN accuracy, and visualized embeddings. However, you didn’t implement UMAP, vary its parameters (e.g., n_neighbors, min_dist, n_components), or compare KNN performance vs PCA. Add UMAP runs and visualizations to complete.

**Part ex2-part2** (ex2-part2.code): 3/7 points

_Feedback:_ You implemented UMAP with KNN and clear visualizations—good technique and likely works. However, the task explicitly asked to try PCA (and to leverage your prior PCA work); you didn’t include PCA here. Explanation is generic and not tied to shown PCA results. Add PCA runs to earn

---

## Additional Information

This feedback was automatically generated by the autograder using LLM-based evaluation.

**Generated:** 2025-10-27 18:51:24 UTC

If you have questions about your grade, please reach out to the instructor.

---

*Powered by [Grade-Lite](https://github.com/your-repo/grade-lite) Autograder*